import logging, os
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
from tensorflow.python.keras import layers
from tensorflow.python import keras
import tensorflow as tf
import tensorflow_probability as tfp
import matplotlib.pyplot as plt
import matplotlib
from queue import Queue
import numpy as np
import multiprocessing
import re
from utils import PPrint
import argparse
import json
import utils
import threading
import time
import sys
import config
import os

os.environ["CUDA_VISIBLE_DEVICES"] = ""
sys.path.append(config.PROJECT_PATH + "/classes/")
import msf_wrapper
import a3c_classes
import scan_cluster
import queue
import service_scan
import generate_exploits_tree
matplotlib.use('PS')
tf.compat.v1.enable_eager_execution()
ST_OS_TYPE = 1    # OS types (unix, linux, windows, osx..).
ST_SERV_NAME = 0  # Product name on Port.
NUM_STATES = 1    # Size of state.
TOTAL_EPS = 1000  # Number of episodes
NUM_WORKERS = 1   # Number of workers
OS_LIST = config.OS_LIST.split("@")
SERVICE_LIST = config.SERVICE_LIST.split("@")
exploits_array = []
global_episode = 0


def build_model(state_size, action_space):
    input_layer = layers.Input(batch_shape=(None, state_size))
    dense_layer1 = layers.Dense(10, activation='relu')(input_layer)
    dense_layer2 = layers.Dense(20, activation='relu')(dense_layer1)
    dense_layer3 = layers.Dense(20, activation='relu')(dense_layer2)
    dense_layer4 = layers.Dense(40, activation='relu')(dense_layer3)
    out_actions = layers.Dense(
        action_space, activation='softmax')(dense_layer4)
    out_value = layers.Dense(1, activation='linear')(dense_layer4)
    model = keras.Model(inputs=[input_layer], outputs=[out_actions, out_value])
    model.make_predict_function()
    return model


def record(episode, episode_reward, worker_idx, global_ep_reward, result_queue, total_loss, num_steps):
    global_ep_reward = episode_reward
    print(
        f"Episode: {episode} | "
        f"Moving Average Reward: {int(global_ep_reward)} | "
        f"Episode Reward: {int(episode_reward)} | "
        f"Loss: {int(total_loss / float(num_steps) * 1000) / 1000} | "
        f"Steps: {num_steps} | "
        f"Worker: {worker_idx}"
    )
    result_queue.put(global_ep_reward)
    return global_ep_reward


class Environment():
    def __init__(self, name):
        self.host = name
        self.state = []

    def normalization(self, target_idx):
        if target_idx == ST_OS_TYPE:
            os_num = int(self.state[ST_OS_TYPE])
            os_num_mean = len(OS_LIST) / 2.0
            self.state[ST_OS_TYPE] = (os_num - os_num_mean) / os_num_mean
        if target_idx == ST_SERV_NAME:
            service_num = self.state[ST_SERV_NAME]
            service_num_mean = len(SERVICE_LIST) / 2.0
            self.state[ST_SERV_NAME] = (
                service_num - service_num_mean) / service_num_mean

    def run_testing_worker(self, input_data, action):
        exploit = utils.get_value(exploits_array, action)
        try:
            test_output = utils.get_exploit_reward(input_data, exploit)
        except:
            test_output = {"host": None}
        if test_output["host"]:
            PPrint().success("Pwned!")
            return test_output
        return None

    def get_state(self, input_data):
        self.state = []
        # Set os type to state.
        # os = input_data["platform"] or ""
        # self.state.insert(ST_OS_TYPE, utils.get_index(OS_LIST, os.lower()))
        # self.normalization(ST_OS_TYPE)
        # Get product name.
        service_name = input_data['name'].replace(" ", "") or ""
        self.state.insert(ST_SERV_NAME, utils.get_index(
            SERVICE_LIST, service_name.lower()))
        # self.normalization(ST_SERV_NAME)
        return np.asarray(self.state)

    def reset(self, details):
        # Initialize state.
        self.state = []
        # os = details['platform'] or ""
        # Set os type to state.
        # self.state.insert(ST_OS_TYPE, utils.get_index(OS_LIST, os.lower()))
        # Get product name.

        # self.normalization(ST_OS_TYPE)
        service_name = details['name'].replace(" ", "")
        self.state.insert(ST_SERV_NAME, utils.get_index(
            SERVICE_LIST, service_name.lower()))
        # self.normalization(ST_SERV_NAME)
        return np.asarray(self.state)

    def step(self, input_data, action):
        result = self.run_testing_worker(input_data, action)
        done = True  # always true.
        if not result:
            reward = -1
        else:
            reward = 1
        state = self.get_state(input_data)
        return state, reward, done


class MasterAgent():
    def __init__(self, host):
        global exploits_array
        self.host = host
        save_dir = "./"
        self.save_dir = save_dir
        if not os.path.exists(save_dir):
            os.makedirs(save_dir)
        env = Environment(host)
        self.state_size = NUM_STATES
        client = config.getClient()
        exploits_array = config.loadExploitsTree(detailed=False)
        self.action_size = len(exploits_array)
        self.opt = tf.keras.optimizers.Adam(0.9)
        self.global_model = build_model(
            self.state_size, self.action_size)  # global network
        self.global_model(tf.convert_to_tensor(
            np.random.random((1, self.state_size)), dtype=tf.float32))
        try:
            model_path = os.path.join(self.save_dir, 'pwn_model.h5')
            print('Loading model from: {}'.format(model_path))
            self.global_model.load_weights(model_path)
        except:
            # Doesn't exist
            pass

    def train(self, result):
        global exploits_array
        res_queue = Queue()
        port_groups = utils.generate_chunks(result['ports'], NUM_WORKERS)
        workers = [Worker(self.state_size,
                          self.action_size,
                          self.global_model,
                          self.opt, res_queue,
                          i, host=self.host,
                          save_dir=self.save_dir, result=result, ports=port_group) for i, port_group in enumerate(port_groups)]
        for i, worker in enumerate(workers):
            print("Starting worker {}".format(i))
            worker.start()

        moving_average_rewards = []  # record episode reward to plot
        while True:
            reward = res_queue.get()
            if reward is not None:
                moving_average_rewards.append(reward)   
            else:
                break

        [w.join() for w in workers]

        # Saving the model when finally done.
        self.global_model.save_weights(
            os.path.join(self.save_dir,
                         'pwn_model.h5')
        )

    def get_suggested_exploits(self, model, port, result):
        port_exploits = []
        data = result["service_details"][str(port)]
        input_data = {"host": self.host, "port": port,
                      "name": data["name"], "name": data["name"], "version": data["version"], "platform": result["osname"]}
        env = Environment(self.host)
        state = env.reset(input_data)
        reward_sum = 0
        p, _ = model.predict(state)
        prob = []
        for ind in range(len(exploits_array)):
            action = utils.get_value(exploits_array, ind)
            prob.append([action, p[0][ind]])
        prob.sort(key=lambda s: -s[1])
        for i in prob[:20]:
            action = i[0]
            if scan_cluster.post_process_exploit_suggestion(port, result["osname"], data["name"], action):
                port_exploits.append(action)
            if len(port_exploits) >= 4:  # TOP 4 exploits
                break
        return port_exploits

    def play(self, result):
        model = self.global_model
        model_path = os.path.join(self.save_dir, 'pwn_model.h5')
        print('Loading model from: {}'.format(model_path))
        model.load_weights(model_path)
        exploits_list = []
        for port in result["ports"]:
            port_exploits = self.get_suggested_exploits(model, port, result)
            exploits_list.append((port, port_exploits))
        return exploits_list


class Memory:
    def __init__(self):
        self.states = []
        self.actions = [1]
        self.rewards = []
        self.old_log_probs = []

    def store(self, state, action, reward):
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)

    def clear(self):
        self.states = []
        self.actions = []
        self.rewards = []


class Worker(threading.Thread):
    # Set up global variables across different threads
    # Moving average reward
    global_moving_average_reward = 0
    best_score = 0
    save_lock = threading.Lock()

    def __init__(self,
                 state_size,
                 action_size,
                 global_model,
                 opt,
                 result_queue,
                 idx,
                 host='',
                 save_dir='./',
                 result={},
                 ports=[]
                 ):
        super(Worker, self).__init__()
        global global_episode
        self.state_size = state_size
        self.action_size = action_size
        self.result_queue = result_queue
        self.global_model = global_model
        self.opt = opt
        self.local_model = build_model(self.state_size, self.action_size)
        self.worker_idx = idx
        self.host = host
        self.env = Environment(self.host)
        self.save_dir = save_dir
        self.ep_loss = 0.0
        self.result = result
        self.input_data = {}
        self.exploits_list = []
        self.ports = ports

    def run(self):
        global global_episode
        global exploits_array
        index_states = 1
        while global_episode < TOTAL_EPS:
            # print('global_epsilon', global_episode)
            for port in self.ports:
                # print('port', port)
                data = self.result["service_details"][str(port)]
                self.exploits_list = data['exploits']
                self.input_data = {"host": self.host, "port": port,
                                   "name": data["name"], "name": data["name"], "version": data["version"], "platform": self.result["osname"]}
                current_state = self.env.reset(self.input_data)
                mem = Memory()
                ep_reward = 0.
                ep_steps = 1
                self.ep_loss = 0
                if len(data['exploits']):
                    action = utils.get_index(
                        exploits_array, np.random.choice(data['exploits']))
                    # print('dieu kien dung')
                else:
                    # print('dieu kien sai')
                    logitss,  _ = self.local_model(tf.convert_to_tensor(
                        current_state[None, :], dtype=tf.float32))
                    probss = tf.nn.softmax(logitss)
                    #print(probss)
                    action = np.random.choice(
                        self.action_size, p=probss.numpy()[0])
                new_state, reward, done = self.env.step(
                    self.input_data, action)
                ep_reward += reward
                mem.clear()

                mem.store(current_state, action, reward)
                
                    
                # Calculate gradient wrt to local model. We do so by tracking the
                # variables involved in computing the loss by using tf.GradientTape
                # print(global_episode)
                # # print(index_states)
                # start_states = index_states - 1
                # end_states = start_states + 49
                # index_states += 50
                
                # # logits, values = self.local_model(
                # #                 tf.convert_to_tensor(np.vstack(mem.states[start_states:end_states]),
                # #                 dtype=tf.float32))
                # # policy = tf.nn.softmax(logits)
                # old_logits, _ = self.local_model(tf.convert_to_tensor(mem.states[0:25], dtype=tf.float32))
                # old_policy = tfp.distributions.Categorical(logits=old_logits)
                # old_log_probs = old_policy.log_prob(mem.actions[0:25])
            
                # old_logits, _ = self.local_model(tf.convert_to_tensor((mem.states[start_states - 50: end_states - 50]),
                #                                                                                                 dtype=tf.float32))
                # old_policy = tfp.distributions.Categorical(logits=old_logits)
                # old_log_probs = old_policy.log_prob(mem.actions[start_states-50:end_states-50])
                # # categorical = tfp.distributions.Categorical(probs=old_policy)
                
                
                # #print(mem.states[0:10][0])
                # #print(mem.actions)
                # clip_epsilon = 0.005
                # epsilon = 0.2
                # gamma = 0.8
                # old_action = mem.actions[-2]
                # new_action = mem.actions[-1]   
                # print('old_action', old_action) 
                # print('new_acion', new_action)
                
                with tf.GradientTape() as tape: 
                    # total_loss = self.compute_loss(mem, new_state, new_action, old_action, clip_epsilon, epsilon, gamma)
                    # print(total_loss)
                    total_loss = self.compute_loss(new_state, mem)
                    self.ep_loss += total_loss
                    # # Calculate local gradients
                    # grads = tape.gradient(
                    #     total_loss, self.local_model.trainable_weights)
                    # # Push local gradients to global model
                    
                    
                    grads = tape.gradient(
                        total_loss, self.local_model.trainable_weights)
                
                    
                self.opt.apply_gradients(zip(grads, self.global_model.trainable_weights)) 
                # Update local model with new weights
                self.local_model.set_weights(self.global_model.get_weights())
                
                
                
                if reward > 0:
                    global_episode += 1
                    Worker.global_moving_average_reward = \
                        record(global_episode, ep_reward, self.worker_idx,
                            Worker.global_moving_average_reward, self.result_queue,
                            self.ep_loss, ep_steps)
            
                ep_steps += 1
                # if len(mem.actions) > 6:
                #     mem.actions.pop(0)
                # print('ep_steos:', ep_steps)
                mem.clear()
            

                
        self.result_queue.put(None)

    # def compute_loss(self,
    #                  mem,
    #                  state, old_action,
    #                  new_action,
    #                  clip_epsilon,
    #                  epsilon,
    #                  gamma):
    #     # print('new_state: ', new_state)
    #     # print('current_state: ', current_state)
    #     # print('action', action)
    #     reward_sum = 0.
    #                 # Get discounted rewards
    #     discounted_rewards = []
    #     for reward in mem.rewards[::-1]:  # reverse buffer r
    #         reward_sum = reward + gamma * reward_sum
    #         discounted_rewards.append(reward_sum)
    #     discounted_rewards.reverse()
    #     # old_logits, _ = self.local_model(tf.convert_to_tensor(current_state[None, :], dtype=tf.float32))
    #     # old_probs = tf.nn.softmax(old_logits)
    #     # old_log_prob = tf.reduce_sum(tf.math.log(old_probs) * action, axis=1)

    #     logits, values = self.local_model(tf.convert_to_tensor(
    #         state[None, :], dtype=tf.float32))
    #     # probs = tf.nn.softmax(logits)
    #     # new_action = np.random.choice(
    #     #     self.action_size, p=probs.numpy()[0])
    #     # print('new_action', new_action)

    #     # categorical = tfp.distributions.Categorical(probs=probs)
    #     # log_probs = categorical.log_prob(new_action)

    #     # ratio = tf.exp(log_probs - old_log_prob)
    #     A_old = old_action
    #     if A_old == 0:
    #         A_old += 1
    #     A_new = new_action
    #     ratio = A_new / A_old            
    #     clip_ratio = tf.clip_by_value(ratio, clip_value_min=1-epsilon, clip_value_max=1+epsilon)

    #     advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],
    #                             dtype=tf.float32) - values

    #     total_loss = -tf.minimum(ratio * advantage, clip_ratio * advantage)
    #     # surrogate_loss = tf.minimum(ratio * advantages, tf.clip_by_value(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantages)
    #     # policy_loss = -tf.reduce_mean(surrogate_loss)
    #     # entropy_loss = -tf.reduce_mean(probs * tf.math.log(probs + 1e-10), axis=1)
    #     # total_loss = policy_loss + tf.reduce_mean(epsilon * entropy_loss)
    #     # Get discounted rewards
    #     # discounted_rewards = []
    #     # for reward in memory.rewards[::-1]:  # reverse buffer r
    #     #     reward_sum = reward + gamma * reward_sum
    #     #     discounted_rewards.append(reward_sum)
    #     # discounted_rewards.reverse()

    #     # logits, values = self.local_model(
    #     #     tf.convert_to_tensor(states,
    #     #                          dtype=tf.float32))
    #     # # Get our advantages
    #     # advantage = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],
    #     #                                  dtype=tf.float32) - values
    #     # # Value losscategorical = tfp.distributions.Categorical(probs=policy)
    #     # log_probs = categorical.log_prob(actions)
    #     # ratio = tf.exp(log_probs - old_log_probs)
    #     # value_loss = advantage ** 2
    #     # policy = tf.nn.softmax(logits)
    #     # categorical = tfp.distributions.Categorical(probs=policy)
    #     # log_probs = categorical.log_prob(actions)
    #     # ratio = tf.exp(log_probs - old_log_probs)
    #     # surrogate_lossses = tf.minimum(ratio * advantage, tf.clip_by_value(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * advantage )
    #     # # Calculate our policy loss
    #     # # actions_one_hot = tf.one_hot(
    #     # #     memory.actions, self.action_size, dtype=tf.float32)
        
    #     # # entropy = tf.reduce_sum(policy * tf.math.log(policy + 1e-10), axis=1)
    #     # # policy_loss = tf.nn.softmax_cross_entropy_with_logits(
    #     # #     labels=actions_one_hot, logits=logits)
    #     # # total_loss = tf.reduce_mean((value_loss + policy_loss))
    #     # policy_loss *= tf.stop_gradient(advantage)
    #     # # policy_loss += 0.01 * entropy
    #     # # total_loss = tf.reduce_mean(((0.5 * value_loss) + policy_loss))
    #     # total_loss = -tf.reduce_mean(surrogate_lossses)
    #     return total_loss

    def compute_loss(self, new_state, memory, gamma=0.99, clip_epsilon=0.2):

        logits, values = self.local_model(
            tf.convert_to_tensor(np.vstack(new_state),
                                dtype=tf.float32))
        old_logits, _ = self.local_model(
            tf.convert_to_tensor(np.vstack(memory.states),
                                dtype=tf.float32))
        

        # Get discounted rewards
        discounted_rewards = []
        reward_sum = 0.
        for reward in memory.rewards[::-1]:  # reverse buffer r
            reward_sum = reward + gamma * reward_sum
            discounted_rewards.append(reward_sum)
        discounted_rewards.reverse()
        advantages = tf.convert_to_tensor(np.array(discounted_rewards)[:, None],
                                        dtype=tf.float32) - values

        # Calculate our policy loss
        actions_one_hot = tf.one_hot(memory.actions, self.action_size,
                                    dtype=tf.float32)
        policy = tf.nn.softmax(logits)
        old_policy = tf.nn.softmax(old_logits)
        entropy = -tf.reduce_mean(tf.reduce_sum(policy * tf.math.log(policy + 1e-10), axis=1))

        ratios = tf.exp(tf.nn.log_softmax(logits) - tf.nn.log_softmax(old_logits))
        surr1 = ratios * advantages
        surr2 = tf.clip_by_value(ratios, 1.0 - clip_epsilon, 1.0 + clip_epsilon) * advantages
        policy_loss = -tf.reduce_mean(tf.minimum(surr1, surr2))

        total_loss = policy_loss + 0.5 * tf.reduce_mean(tf.square(advantages)) - 0.01 * entropy

        return total_loss